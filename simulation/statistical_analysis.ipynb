{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_multi_queue = pd.read_csv('results_directory/same_patient_profiles/multi_queue/current_state/averages_data.csv')\n",
    "averages_single_queue = pd.read_csv('results_directory/same_patient_profiles/single_queue/current_state/averages_data.csv')\n",
    "\n",
    "# calculate the mean of the averages\n",
    "mean_multi_queue = averages_multi_queue.mean()\n",
    "mean_single_queue = averages_single_queue.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we compare both simulations which were ran with identical patient profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentially, we have a sample of pairs $$(X_1, Y_1), (X_2, Y_2),\\dots, (X_{100}, Y_{100})$$\n",
    "### We can now create a sample of differences $$D_i = (X_1 - Y_i)$$\n",
    "### Now $D_1, D_2, \\dots D_{100}$ are i.i.d. \n",
    "### Additionally, $$\\mathbf{E}(D_i) = \\mathbf{E}(X_i) - \\mathbf{E}(Y_i)$$\n",
    "### Our only assumption from here on out will be that $D_i\\sim\\mathcal{N}(\\mathbf{E}(D_i), \\mathbf{Var}(D_i))$\n",
    "### This is not a groundless assumption, since, for sufficiently large $n$, $X$ and $Y$, which are averages, should be approximatelly normally distributed. as such, their difference, as a linear function of two normally distributed variables, would also be normally distributed.\n",
    "\n",
    "#### Credit to Technion statistics 1 course - presentation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to create two dfs like the ones loaded but without columns containing the word \"nurse\"\n",
    "nurse_columns_multi_queue = [col for col in averages_multi_queue.columns if \"nurse\" in col]\n",
    "nurse_columns_single_queue = [col for col in averages_single_queue.columns if \"nurse\" in col]\n",
    "\n",
    "no_nurse_multi_queue = averages_multi_queue.drop(columns=nurse_columns_multi_queue)\n",
    "no_nurse_single_queue = averages_single_queue.drop(columns=nurse_columns_single_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences_df = no_nurse_multi_queue - no_nurse_single_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'q_flow_station_wait_time_avg',\n",
       "       'q_flow_station_queue_length_avg', 'q_flow_station_busy_avg',\n",
       "       'secretary_station_wait_time_avg', 'secretary_station_queue_length_avg',\n",
       "       'secretary_station_busy_avg', 'leukemia_doctor_1_wait_time_avg',\n",
       "       'leukemia_doctor_1_queue_length_avg', 'leukemia_doctor_1_busy_avg',\n",
       "       'leukemia_doctor_2_wait_time_avg', 'leukemia_doctor_2_queue_length_avg',\n",
       "       'leukemia_doctor_2_busy_avg', 'transplant_doctor_1_wait_time_avg',\n",
       "       'transplant_doctor_1_queue_length_avg', 'transplant_doctor_1_busy_avg',\n",
       "       'transplant_doctor_2_wait_time_avg',\n",
       "       'transplant_doctor_2_queue_length_avg', 'transplant_doctor_2_busy_avg',\n",
       "       'transplant_doctor_3_wait_time_avg',\n",
       "       'transplant_doctor_3_queue_length_avg', 'transplant_doctor_3_busy_avg',\n",
       "       'other_patients_total_processing_time_avg',\n",
       "       'leukemia_doctor_1_complex_patients_total_processing_time_avg',\n",
       "       'leukemia_doctor_1_regular_patients_total_processing_time_avg',\n",
       "       'leukemia_doctor_2_complex_patients_total_processing_time_avg',\n",
       "       'leukemia_doctor_2_regular_patients_total_processing_time_avg',\n",
       "       'transplant_doctor_1_complex_patients_total_processing_time_avg',\n",
       "       'transplant_doctor_1_regular_patients_total_processing_time_avg',\n",
       "       'transplant_doctor_2_complex_patients_total_processing_time_avg',\n",
       "       'transplant_doctor_2_regular_patients_total_processing_time_avg',\n",
       "       'transplant_doctor_3_complex_patients_total_processing_time_avg',\n",
       "       'transplant_doctor_3_regular_patients_total_processing_time_avg',\n",
       "       'leukemia_doctor_1_scheduled_vs_actual_time_diff_avg',\n",
       "       'leukemia_doctor_2_scheduled_vs_actual_time_diff_avg',\n",
       "       'transplant_doctor_1_scheduled_vs_actual_time_diff_avg',\n",
       "       'transplant_doctor_2_scheduled_vs_actual_time_diff_avg',\n",
       "       'transplant_doctor_3_scheduled_vs_actual_time_diff_avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our hypothesis $H_0$ is - $\\mu_d = d_0$.\n",
    "### In our case $n = 100$; $d_0 = 0$\n",
    "\n",
    "### We make two observations:\n",
    "### - Under $H_0$: $T = \\frac{\\bar{D}-d_0}{\\frac{S_D}{\\sqrt{n}}}\\sim t_{(n-1)}$\n",
    "### - $S_D^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(D_i - \\bar{D}\\right)^2$\n",
    "\n",
    "### Thus, a confidence interval at confidence level $1 - \\alpha$ is:\n",
    "### $$\\left[\\bar{D} \\pm t_{(n-1),1-\\frac{\\alpha}{2}}\\frac{S_D}{\\sqrt{n}}\\right]$$\n",
    "\n",
    "#### Credit to Technion statistics 1 course - presentation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S_D(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the sample standard deviation for a given column in a DataFrame.\n",
    "    \"\"\"\n",
    "    return np.std(df[column_name], ddof=1) # ddof=1 to ensure the sample standard deviation is an unbiased estimator of the population standard deviation\n",
    "\n",
    "def D_bar(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the sample mean for a given column in a DataFrame.\n",
    "    \"\"\"\n",
    "    return np.mean(df[column_name])\n",
    "\n",
    "def calculate_confidence_interval(df, column_name, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a given column in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_name (str): The name of the column to calculate the confidence interval for.\n",
    "        confidence_level (float): The confidence level for the interval (default is 0.95).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(df)\n",
    "    t_value = stats.t.ppf(1 - (1 - confidence_level) / 2, n - 1)\n",
    "    lower_bound = D_bar(df, column_name) - t_value * S_D(df, column_name) / np.sqrt(n)\n",
    "    upper_bound = D_bar(df, column_name) + t_value * S_D(df, column_name) / np.sqrt(n)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def does_null_hypothesis_hold(df, column_name, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Check if the null hypothesis holds for a given column in a DataFrame.\n",
    "    \"\"\"\n",
    "    lower_bound, upper_bound = calculate_confidence_interval(df, column_name, confidence_level)\n",
    "    print(f\"Column name: {column_name}\")\n",
    "    print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
    "    print(f\"Null hypothesis holds: {lower_bound <= 0 <= upper_bound}\")\n",
    "    print(f\"{'#'*50}\")\n",
    "    return lower_bound <= 0 <= upper_bound\n",
    "\n",
    "def does_null_hypothesis_hold_for_all_columns(df, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Check if the null hypothesis holds for all columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if not does_null_hypothesis_hold(df, column, confidence_level):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def csv_of_results(df, column_name, confidence_level=0.95):\n",
    "    # Create results dataframe with proper columns\n",
    "    results_df = pd.DataFrame(columns=['lower_bound', 'upper_bound', 'null_hypothesis_holds'])\n",
    "    \n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            lower_bound, upper_bound = calculate_confidence_interval(df, column, confidence_level)\n",
    "            null_hypothesis_result = does_null_hypothesis_hold(df, column, confidence_level)\n",
    "            \n",
    "            # Add row using loc with proper indexing\n",
    "            results_df.loc[column] = {\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound, \n",
    "                'null_hypothesis_holds': null_hypothesis_result\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {column}: {e}\")\n",
    "            # Add row with NaN values if there's an error\n",
    "            results_df.loc[column] = {\n",
    "                'lower_bound': np.nan,\n",
    "                'upper_bound': np.nan,\n",
    "                'null_hypothesis_holds': np.nan\n",
    "            }\n",
    "    \n",
    "    results_df.to_csv(f\"results_directory/statistical_analysis/{column_name}.csv\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name: Unnamed: 0\n",
      "Lower bound: 0.0, Upper bound: 0.0\n",
      "Null hypothesis holds: True\n",
      "##################################################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcsv_of_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdifferences_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferences_df\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mcsv_of_results\u001b[39m\u001b[34m(df, column_name, confidence_level)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     58\u001b[39m     lower_bound, upper_bound = calculate_confidence_interval(df, column, confidence_level)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mresults_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m = [lower_bound, upper_bound, does_null_hypothesis_hold(df, column, confidence_level)]\n\u001b[32m     60\u001b[39m results_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults_directory/statistical_analysis/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/technion/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:911\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    910\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/technion/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1932\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1929\u001b[39m     indexer, missing = convert_missing_indexer(indexer)\n\u001b[32m   1931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m-> \u001b[39m\u001b[32m1932\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1933\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1935\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mloc\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1936\u001b[39m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/technion/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:2306\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer_missing\u001b[39m\u001b[34m(self, indexer, value)\u001b[39m\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[32m   2304\u001b[39m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[32m   2305\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj.columns):\n\u001b[32m-> \u001b[39m\u001b[32m2306\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot set a row with mismatched columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2308\u001b[39m     value = Series(value, index=\u001b[38;5;28mself\u001b[39m.obj.columns, name=indexer)\n\u001b[32m   2310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj):\n\u001b[32m   2311\u001b[39m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[32m   2312\u001b[39m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "csv_of_results(differences_df, \"differences_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name: Unnamed: 0\n",
      "Lower bound: 0.0, Upper bound: 0.0\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: q_flow_station_wait_time_avg\n",
      "Lower bound: -0.044476209632849664, Upper bound: 0.04768416744189949\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: q_flow_station_queue_length_avg\n",
      "Lower bound: -0.006190289296514656, Upper bound: 0.004386939962445271\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: q_flow_station_busy_avg\n",
      "Lower bound: -0.009190910781360776, Upper bound: 0.001379415212729664\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: secretary_station_wait_time_avg\n",
      "Lower bound: -0.48569281542137643, Upper bound: 0.20516260291974017\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: secretary_station_queue_length_avg\n",
      "Lower bound: -0.06857685140439146, Upper bound: 0.016399777771461822\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: secretary_station_busy_avg\n",
      "Lower bound: -0.04068511994877346, Upper bound: 0.009936396864538581\n",
      "Null hypothesis holds: True\n",
      "##################################################\n",
      "Column name: leukemia_doctor_1_wait_time_avg\n",
      "Lower bound: -32.5258781177605, Upper bound: -9.228535959150793\n",
      "Null hypothesis holds: False\n",
      "##################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "does_null_hypothesis_hold_for_all_columns(differences_df, 0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
